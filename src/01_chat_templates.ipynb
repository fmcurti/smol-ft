{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1f8693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/smolcourse/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "GPU memory: 4.3GB\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Apple MPS\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU - you will need to use a GPU to train models\")\n",
    "\n",
    "# Authenticate with Hugging Face (optional, for private models)\n",
    "from huggingface_hub import login\n",
    "# login()  # Uncomment if you need to access private models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5743d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "instruct_model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "# Load tokenizers\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "\n",
    "# Load models (use smaller precision for memory efficiency)\n",
    "\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff35c5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SIMPLE_QA ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- WITH_SYSTEM ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant specialized in explaining technical concepts clearly.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant specialized in explaining technical concepts clearly.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- MULTI_TURN ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a math tutor.\n",
      "\n",
      "<|im_start|>user\n",
      "What is calculus?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a simple example?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a math tutor.\n",
      "\n",
      "<|im_start|>user\n",
      "What is calculus?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a simple example?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- REASONING_TASK ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face.\n",
      "\n",
      "<|im_start|>user\n",
      "Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face.\n",
      "\n",
      "<|im_start|>user\n",
      "Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create different types of conversations to test\n",
    "conversations = {\n",
    "    \"simple_qa\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "    ],\n",
    "    \n",
    "    \"with_system\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in explaining technical concepts clearly.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "    ],\n",
    "    \n",
    "    \"multi_turn\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is calculus?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you give me a simple example?\"},\n",
    "    ],\n",
    "    \n",
    "    \"reasoning_task\": [\n",
    "        {\"role\": \"user\", \"content\": \"Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "for conv_type, messages in conversations.items():\n",
    "    print(f\"--- {conv_type.upper()} ---\")\n",
    "    \n",
    "    # Format without generation prompt (for completed conversations)\n",
    "    formatted_complete = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    \n",
    "    # Format with generation prompt (for inference)\n",
    "    formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    \n",
    "    print(\"Complete conversation format:\")\n",
    "    print(formatted_complete)\n",
    "    print(\"\\nWith generation prompt:\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca93a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model comparison ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "Instruct model response:\n",
      "nowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /no_think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face.\n",
      "\n",
      "user\n",
      "Explain quantum computing in simple terms. Don't think\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "Quantum computing is a way of doing calculations that\n"
     ]
    }
   ],
   "source": [
    "# Test the same prompt on both models\n",
    "test_prompt = \"Explain quantum computing in simple terms. Don't think\"\n",
    "\n",
    "# Prepare the prompt for base model (no chat template)\n",
    "\n",
    "# Prepare the prompt for instruct model (with chat template)\n",
    "instruct_messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "instruct_formatted = instruct_tokenizer.apply_chat_template(\n",
    "    instruct_messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "instruct_inputs = instruct_tokenizer(instruct_formatted, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate responses\n",
    "print(\"=== Model comparison ===\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Instruct model response:\")\n",
    "with torch.no_grad():\n",
    "    instruct_outputs = instruct_model.generate(\n",
    "        **instruct_inputs,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=instruct_tokenizer.eos_token_id\n",
    "    )\n",
    "    instruct_response = instruct_tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    assistant_start = instruct_response.find(\"<|im_start|>assistant\\n\") + len(\"<|im_start|>assistant\\n\")\n",
    "    assistant_response = instruct_response[assistant_start:].split(\"<|im_end|>\")[0]\n",
    "    print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285155e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0minstruct_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconversation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mchat_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_assistant_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mconversation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mchat_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_assistant_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\u001b[0m\n",
      "\u001b[0;34m        ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\u001b[0m\n",
      "\u001b[0;34m        determine the format and control tokens to use when converting.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            conversation (Union[list[dict[str, str]], list[list[dict[str, str]]]]): A list of dicts\u001b[0m\n",
      "\u001b[0;34m                with \"role\" and \"content\" keys, representing the chat history so far.\u001b[0m\n",
      "\u001b[0;34m            tools (`list[Union[Dict, Callable]]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                A list of tools (callable functions) that will be accessible to the model. If the template does not\u001b[0m\n",
      "\u001b[0;34m                support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\u001b[0m\n",
      "\u001b[0;34m                giving the name, description and argument types for the tool. See our\u001b[0m\n",
      "\u001b[0;34m                [tool use guide](https://huggingface.co/docs/transformers/en/chat_extras#passing-tools)\u001b[0m\n",
      "\u001b[0;34m                for more information.\u001b[0m\n",
      "\u001b[0;34m            documents (`list[dict[str, str]]`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                A list of dicts representing documents that will be accessible to the model if it is performing RAG\u001b[0m\n",
      "\u001b[0;34m                (retrieval-augmented generation). If the template does not support RAG, this argument will have no\u001b[0m\n",
      "\u001b[0;34m                effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.\u001b[0m\n",
      "\u001b[0;34m            chat_template (`str`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                A Jinja template to use for this conversion. It is usually not necessary to pass anything to this\u001b[0m\n",
      "\u001b[0;34m                argument, as the model's template will be used by default.\u001b[0m\n",
      "\u001b[0;34m            add_generation_prompt (bool, *optional*):\u001b[0m\n",
      "\u001b[0;34m                If this is set, a prompt with the token(s) that indicate\u001b[0m\n",
      "\u001b[0;34m                the start of an assistant message will be appended to the formatted output. This is useful when you want to generate a response from the model.\u001b[0m\n",
      "\u001b[0;34m                Note that this argument will be passed to the chat template, and so it must be supported in the\u001b[0m\n",
      "\u001b[0;34m                template for this argument to have any effect.\u001b[0m\n",
      "\u001b[0;34m            continue_final_message (bool, *optional*):\u001b[0m\n",
      "\u001b[0;34m                If this is set, the chat will be formatted so that the final\u001b[0m\n",
      "\u001b[0;34m                message in the chat is open-ended, without any EOS tokens. The model will continue this message\u001b[0m\n",
      "\u001b[0;34m                rather than starting a new one. This allows you to \"prefill\" part of\u001b[0m\n",
      "\u001b[0;34m                the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\u001b[0m\n",
      "\u001b[0;34m            tokenize (`bool`, defaults to `True`):\u001b[0m\n",
      "\u001b[0;34m                Whether to tokenize the output. If `False`, the output will be a string.\u001b[0m\n",
      "\u001b[0;34m            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\u001b[0m\n",
      "\u001b[0;34m                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\u001b[0m\n",
      "\u001b[0;34m                 index) among:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\u001b[0m\n",
      "\u001b[0;34m                  sequence if provided).\u001b[0m\n",
      "\u001b[0;34m                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\u001b[0m\n",
      "\u001b[0;34m                  acceptable input length for the model if that argument is not provided.\u001b[0m\n",
      "\u001b[0;34m                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\u001b[0m\n",
      "\u001b[0;34m                  lengths).\u001b[0m\n",
      "\u001b[0;34m            truncation (`bool`, defaults to `False`):\u001b[0m\n",
      "\u001b[0;34m                Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\u001b[0m\n",
      "\u001b[0;34m            max_length (`int`, *optional*):\u001b[0m\n",
      "\u001b[0;34m                Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\u001b[0m\n",
      "\u001b[0;34m                not specified, the tokenizer's `max_length` attribute will be used as a default.\u001b[0m\n",
      "\u001b[0;34m            return_tensors (`str` or [`~utils.TensorType`], *optional*):\u001b[0m\n",
      "\u001b[0;34m                If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\u001b[0m\n",
      "\u001b[0;34m                values are:\u001b[0m\n",
      "\u001b[0;34m                - `'tf'`: Return TensorFlow `tf.Tensor` objects.\u001b[0m\n",
      "\u001b[0;34m                - `'pt'`: Return PyTorch `torch.Tensor` objects.\u001b[0m\n",
      "\u001b[0;34m                - `'np'`: Return NumPy `np.ndarray` objects.\u001b[0m\n",
      "\u001b[0;34m                - `'jax'`: Return JAX `jnp.ndarray` objects.\u001b[0m\n",
      "\u001b[0;34m            return_dict (`bool`, defaults to `False`):\u001b[0m\n",
      "\u001b[0;34m                Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\u001b[0m\n",
      "\u001b[0;34m            tokenizer_kwargs (`dict[str: Any]`, *optional*): Additional kwargs to pass to the tokenizer.\u001b[0m\n",
      "\u001b[0;34m            return_assistant_tokens_mask (`bool`, defaults to `False`):\u001b[0m\n",
      "\u001b[0;34m                Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\u001b[0m\n",
      "\u001b[0;34m                the mask will contain 1. For user and system tokens, the mask will contain 0.\u001b[0m\n",
      "\u001b[0;34m                This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\u001b[0m\n",
      "\u001b[0;34m            **kwargs: Additional kwargs to pass to the template renderer. Will be accessible by the chat template.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            `Union[list[int], Dict]`: A list of token ids representing the tokenized chat so far, including control tokens. This\u001b[0m\n",
      "\u001b[0;34m            output is ready to pass to the model, either directly or via methods like `generate()`. If `return_dict` is\u001b[0m\n",
      "\u001b[0;34m            set, will return a dict of tokenizer outputs instead.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"of tokenizer outputs to return.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`return_assistant_tokens_mask=True` is incompatible with `return_dict=False`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mis_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mis_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"continue_final_message and add_generation_prompt are not compatible. Use continue_final_message when you want the model to continue the final message, and add_generation_prompt when you want to add a header that will prompt it to start a new assistant message instead.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"continue_final_message is not compatible with return_assistant_tokens_mask.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemplate_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# kwargs overwrite special tokens if both are present\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrendered_chat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_jinja_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mconversations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconversations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mchat_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreturn_assistant_tokens_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_assistant_tokens_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mtemplate_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mrendered_chat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendered_chat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mrendered_chat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0massistant_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mis_batched\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mcurrent_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mfor\u001b[0m \u001b[0massistant_start_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massistant_end_char\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeneration_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mstart_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massistant_start_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mend_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massistant_end_char\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0;32mif\u001b[0m \u001b[0mstart_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                \u001b[0;31m# start_token is out of bounds maybe due to truncation.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mend_token\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                \u001b[0mcurrent_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0massistant_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_batched\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0massistant_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massistant_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"assistant_masks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massistant_masks\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mrendered_chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/smolcourse/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "instruct_tokenizer.apply_chat_template??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608d8363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING REASONING CAPABILITIES ===\n",
      "\n",
      "Problem 1: What is 15 × 24? Show your work.\n",
      "Answer: nowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "user\n",
      "What is 15 × 24? Show your work.\n",
      "assistant\n",
      "<think>\n",
      "Okay, so I need to figure out\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Problem 2: A recipe calls for 2 cups of flour for 12 cookies. How much flour is needed for 30 cookies?\n",
      "Answer: nowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "user\n",
      "A recipe calls for 2 cups of flour for 12 cookies. How much flour is needed for 30 cookies?\n",
      "assistant\n",
      "<think>\n",
      "Okay, so the problem is: A\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Problem 3: If I have $50 and spend $18.75 on lunch and $12.30 on a book, how much money do I have left?\n",
      "Answer: nowledge Cutoff Date: June 2025\n",
      "Today Date: 08 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "user\n",
      "If I have $50 and spend $18.75 on lunch and $12.30 on a book, how much money do I have left?\n",
      "assistant\n",
      "<think>\n",
      "Okay, let's see. The problem\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test SmolLM3's reasoning capabilities\n",
    "reasoning_prompts = [\n",
    "    \"What is 15 × 24? Show your work.\",\n",
    "    \"A recipe calls for 2 cups of flour for 12 cookies. How much flour is needed for 30 cookies?\",\n",
    "    \"If I have $50 and spend $18.75 on lunch and $12.30 on a book, how much money do I have left?\"\n",
    "]\n",
    "\n",
    "print(\"=== TESTING REASONING CAPABILITIES ===\\n\")\n",
    "\n",
    "for i, prompt in enumerate(reasoning_prompts, 1):\n",
    "    print(f\"Problem {i}: {prompt}\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = instruct_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = instruct_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.3,  # Lower temperature for more consistent reasoning\n",
    "            do_sample=True,\n",
    "            pad_token_id=instruct_tokenizer.eos_token_id\n",
    "        )\n",
    "        response = instruct_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_start = response.find(\"<|im_start|>assistant\\n\") + len(\"<|im_start|>assistant\\n\")\n",
    "        assistant_response = response[assistant_start:].split(\"<|im_end|>\")[0]\n",
    "        print(f\"Answer: {assistant_response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7bd2b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{# ───── defaults ───── #}\\n{%- if enable_thinking is not defined -%}\\n{%- set enable_thinking = true -%}\\n{%- endif -%}\\n\\n{# ───── reasoning mode ───── #}\\n{%- if enable_thinking -%}\\n  {%- set reasoning_mode = \"/think\" -%}\\n{%- else -%}\\n  {%- set reasoning_mode = \"/no_think\" -%}\\n{%- endif -%}\\n\\n{# ───── header (system message) ───── #}\\n{{- \"<|im_start|>system\\\\n\" -}}\\n\\n{%- if messages[0].role == \"system\" -%}\\n  {%- set system_message = messages[0].content -%}\\n  {%- if \"/no_think\" in system_message -%}\\n    {%- set reasoning_mode = \"/no_think\" -%}\\n  {%- elif \"/think\" in system_message -%}\\n    {%- set reasoning_mode = \"/think\" -%}\\n  {%- endif -%}\\n  {%- set custom_instructions = system_message.replace(\"/no_think\", \"\").replace(\"/think\", \"\").rstrip() -%}\\n{%- endif -%}\\n\\n{%- if \"/system_override\" in system_message -%}\\n  {{- custom_instructions.replace(\"/system_override\", \"\").rstrip() -}}\\n  {{- \"<|im_end|>\\\\n\" -}}\\n{%- else -%}\\n  {{- \"## Metadata\\\\n\\\\n\" -}}\\n  {{- \"Knowledge Cutoff Date: June 2025\\\\n\" -}}\\n  {%- set today = strftime_now(\"%d %B %Y\") -%}\\n  {{- \"Today Date: \" ~ today ~ \"\\\\n\" -}}\\n  {{- \"Reasoning Mode: \" + reasoning_mode + \"\\\\n\\\\n\" -}}\\n  \\n  {{- \"## Custom Instructions\\\\n\\\\n\" -}}\\n  {%- if custom_instructions -%}\\n    {{- custom_instructions + \"\\\\n\\\\n\" -}}\\n  {%- elif reasoning_mode == \"/think\" -%}\\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\\\\n\\\\n\" -}}\\n  {%- else -%}\\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face.\\\\n\\\\n\" -}}\\n  {%- endif -%}\\n\\n  {%- if xml_tools or python_tools or tools -%}\\n    {{- \"### Tools\\\\n\\\\n\" -}}\\n    {%- if xml_tools or tools -%}\\n      {%- if tools -%}\\n        {%- set xml_tools = tools -%}\\n      {%- endif -%}\\n      {%- set ns = namespace(xml_tool_string=\"You may call one or more functions to assist with the user query.\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n\\\\n<tools>\\\\n\") -%}\\n      {%- for tool in xml_tools[:] -%} {# The slicing makes sure that xml_tools is a list #}\\n        {%- set ns.xml_tool_string = ns.xml_tool_string ~ (tool | string) ~ \"\\\\n\" -%}\\n      {%- endfor -%}\\n      {%- set xml_tool_string = ns.xml_tool_string + \"</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call>\" -%}\\n      {{- xml_tool_string -}}\\n    {%- endif -%}\\n    {%- if python_tools -%}\\n      {%- set ns = namespace(python_tool_string=\"When you send a message containing Python code between \\'<code>\\' and \\'</code>\\' tags, it will be executed in a stateful Jupyter notebook environment, and you will then be given the output to continued reasoning in an agentic loop.\\\\n\\\\nYou can use the following tools in your python code like regular functions:\\\\n<tools>\\\\n\") -%}\\n      {%- for tool in python_tools[:] -%} {# The slicing makes sure that python_tools is a list #}\\n        {%- set ns.python_tool_string = ns.python_tool_string ~ (tool | string) ~ \"\\\\n\" -%}\\n      {%- endfor -%}\\n      {%- set python_tool_string = ns.python_tool_string + \"</tools>\\\\n\\\\nThe state persists between code executions: so variables that you define in one step are still available thereafter.\" -%}\\n      {{- python_tool_string -}}\\n    {%- endif -%}\\n    {{- \"\\\\n\\\\n\" -}}\\n    {{- \"<|im_end|>\\\\n\" -}}\\n  {%- endif -%}\\n{%- endif -%}\\n{# ───── main loop ───── #}\\n{%- for message in messages -%}\\n    {%- set content = message.content if message.content is string else \"\" -%}\\n    {%- if message.role == \"user\" -%}\\n        {{ \"<|im_start|>\" + message.role + \"\\\\n\"  + content + \"<|im_end|>\\\\n\" }}\\n    {%- elif message.role == \"assistant\" -%}\\n        {% generation %}\\n        {%- if reasoning_mode == \"/think\" -%}\\n            {{ \"<|im_start|>assistant\\\\n\" + content.lstrip(\"\\\\n\") + \"<|im_end|>\\\\n\" }}\\n        {%- else -%}\\n            {{ \"<|im_start|>assistant\\\\n\" + \"<think>\\\\n\\\\n</think>\\\\n\" + content.lstrip(\"\\\\n\") + \"<|im_end|>\\\\n\" }}\\n        {%- endif -%}\\n        {% endgeneration %}\\n    {%- elif message.role == \"tool\" -%}\\n    {{ \"<|im_start|>\" + \"user\\\\n\"  + content + \"<|im_end|>\\\\n\" }}\\n    {%- endif -%}\\n{%- endfor -%}\\n{# ───── generation prompt ───── #}\\n{%- if add_generation_prompt -%}\\n    {%- if reasoning_mode == \"/think\" -%}\\n        {{ \"<|im_start|>assistant\\\\n\" }}\\n    {%- else -%}\\n        {{ \"<|im_start|>assistant\\\\n\" + \"<think>\\\\n\\\\n</think>\\\\n\"  }}\\n    {%- endif -%}\\n{%- endif -%}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tokenizer.get_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9bdeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
